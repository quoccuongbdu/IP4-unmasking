{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0968a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task: I received a list of about 1500 US investment banks. My tasks are:\n",
    "# 1- identify all IP4s used by those investment banks.\n",
    "# 2- get all EDGAR Log records from the US SEC website to verify those banks' digital footprints.\n",
    "# 3- filter edgar log records associated with those IP4s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e3afa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import urllib.request\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a3d00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_dir= \"./ip4_json_dl/\"\n",
    "if not os.path.exists(download_dir):\n",
    "    os.makedirs(download_dir)\n",
    "\n",
    "\n",
    "def json_dowloader(org):\n",
    "    path = # add path here\n",
    "    root = 'https://ip-netblocks.whoisxmlapi.com/api/v2?apiKey=API KEY HERE'\n",
    "    url=root + org\n",
    "    file=requests.get(url)\n",
    "    data=file.json()\n",
    "    with open(download_dir+org+'.json','w') as f:\n",
    "        json.dump(data,f)\n",
    "\n",
    "# download json files\n",
    "if __name__ == '__main__':\n",
    "    path = #add path\n",
    "    df = pd.read_excel(os.path.join(path, 'filename.xlsx'))\n",
    "\n",
    "    org_list = df['org'].to_list()\n",
    "    print(len(org_list))\n",
    "    print(org_list[:20])\n",
    "\n",
    "    for org in org_list:\n",
    "        try:\n",
    "            f = json_dowloader(org)\n",
    "        except:\n",
    "            log = os.path.join(download_dir, \"ip_dl_log.txt\")\n",
    "            f_log = open(log, \"a\")\n",
    "            f_log.write('download_error:' + org + \"\\n\")\n",
    "            f_log.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b5723d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get IP4 function\n",
    "def get_ip(file):\n",
    "    try:\n",
    "        with open(file, 'r') as f:\n",
    "            data = json.load(f)\n",
    "            search = data['search']\n",
    "            print(search)\n",
    "            count = data['result']['count']\n",
    "            print(count)\n",
    "            if count:\n",
    "                orgs = data['result']['inetnums']\n",
    "                # print(orgs)\n",
    "                ip_ranges = []\n",
    "                modified_dates = []\n",
    "                names = []\n",
    "                emails = []\n",
    "                phones = []\n",
    "                countries = []\n",
    "                cities = []\n",
    "                postcodes = []\n",
    "                addresses = []\n",
    "                sources = []\n",
    "                for i in range(0, len(orgs)):\n",
    "                    ip_range = orgs[i]['inetnum']\n",
    "                    ip_ranges.append(ip_range)\n",
    "                    modified_date = orgs[i]['modified']\n",
    "                    modified_dates.append(modified_date)\n",
    "                    source = orgs[i]['source']\n",
    "                    sources.append(source)\n",
    "                    if orgs[i]['org'] is not None:\n",
    "                        name = orgs[i]['org']['name']\n",
    "                        email = orgs[i]['org']['email']\n",
    "                        phone = orgs[i]['org']['phone']\n",
    "                        country = orgs[i]['org']['country']\n",
    "                        city = orgs[i]['org']['city']\n",
    "                        postal_code = orgs[i]['org']['postalCode']\n",
    "                        address = orgs[i]['org']['address']\n",
    "                    if orgs[i]['org'] is None:\n",
    "                        name = \"NA\"\n",
    "                        email = \"NA\"\n",
    "                        phone = \"NA\"\n",
    "                        country = \"NA\"\n",
    "                        city = \"NA\"\n",
    "                        postal_code = \"NA\"\n",
    "                        address = \"NA\"\n",
    "\n",
    "                    names.append(name)\n",
    "                    emails.append(email)\n",
    "                    phones.append(phone)\n",
    "                    countries.append(country)\n",
    "                    cities.append(city)\n",
    "                    postcodes.append(postal_code)\n",
    "                    addresses.append(address)\n",
    "            df = pd.DataFrame({'registered_name': names,\n",
    "                                   'ip_range': ip_ranges,\n",
    "                                   'source': sources,\n",
    "                                   'modified_date': modified_dates,\n",
    "                                   'email': emails,\n",
    "                                   'phone': phones,\n",
    "                                   'address': addresses,\n",
    "                                   'postal_code': postcodes,\n",
    "                                   'city': cities,\n",
    "                                   'country': countries,\n",
    "                                   })\n",
    "            df['searched_name'] = search\n",
    "            df['found_count'] = count\n",
    "            return df\n",
    "    except:\n",
    "        log = os.path.join(download_dir, \"0_json_read_log.txt\")\n",
    "        f_log = open(log, \"a\")\n",
    "        f_log.write('read_error:' + filename + \"\\n\")\n",
    "        f_log.close()\n",
    "\n",
    "\n",
    "# get IP4        \n",
    "output_dir = # add path here\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "path = # add path here\n",
    "all_files = glob.glob(os.path.join(path, \"*.json\"))\n",
    "for file in all_files:\n",
    "    basename = os.path.basename(file)\n",
    "    filename = basename.replace(\".json\", \"\").strip()\n",
    "    df = get_ip(file)\n",
    "    if df is not None:\n",
    "        df.to_csv(output_dir + filename + \".csv\", index=False)\n",
    "    else:\n",
    "        log = os.path.join(output_dir, \"0_json_extract_log.txt\")\n",
    "        f_log = open(log, \"a\")\n",
    "        f_log.write('no result:' + filename + \"\\n\")\n",
    "        f_log.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d599d091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge all the csv files\n",
    "def merge_csv(path):\n",
    "    all_files = glob.glob(os.path.join(path, \"*.csv\"))\n",
    "    df_each_file = (pd.read_csv(f) for f in all_files)\n",
    "    concatenated_df = pd.concat(df_each_file, ignore_index=True)\n",
    "    concatenated_df.to_csv(os.path.join(path, 'filename.csv'), index=False)\n",
    "\n",
    "# execute\n",
    "path = # add path here\n",
    "merge_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a474b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#phase 4 - filtering\n",
    "if __name__=='__main__':\n",
    "    path = # add path here\n",
    "    df = pd.read_csv(os.path.join(path, 'filename.csv'))\n",
    "    print(df.columns)\n",
    "    print(\"df:\", df.shape) \n",
    "\n",
    "    df2 = df[df['country'] == 'US']\n",
    "    print(\"df2:\", df2.shape)  \n",
    "\n",
    "    df3 = df2[df2['registered_name'] != \" \"]\n",
    "    print(\"df3:\", df3.shape) \n",
    "\n",
    "    df3['address'] = df3['address'].str.replace(\"'\", \"\")\n",
    "    df3['phone'] = df3['phone'].str.replace(\"-\", \" \").replace(\"=\", \"+\")\n",
    "    df3[['ip_start', 'ip_end']] = df3.ip_range.str.split(\"-\", expand=True)\n",
    "    df3['org_id'] = df3.groupby('searched_name').ngroup()  \n",
    "\n",
    "    print(df3.columns)\n",
    "    print(df3)\n",
    "\n",
    "    df3.to_csv(os.path.join(path, 'filename.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7784e885",
   "metadata": {},
   "outputs": [],
   "source": [
    "#phase5 - create IP3 range of each bank because EDGAR log only reviews partial IP4s 123.456.789.xxx\n",
    "if __name__=='__main__':\n",
    "    path = # add path here\n",
    "    df = pd.read_csv(os.path.join(path, 'filename.csv'))\n",
    "    df2 = df[['org_id', 'searched_name', 'registered_name', 'ip_start', 'ip_end']]\n",
    "\n",
    "    ip3_start_col = []\n",
    "    ip3_end_col = []\n",
    "    for index, row in df2.iterrows():\n",
    "        if (len(row['ip_start']) <= 16) & len(row['ip_end']) <= 16:\n",
    "            print(index)\n",
    "            ip3_start_l = row['ip_start'].split('.')\n",
    "            ip3_end_l = row['ip_end'].split('.')\n",
    "            ip3_start = \".\".join(ip3_start_l[:3]).strip(\" \")\n",
    "            ip3_end = \".\".join(ip3_end_l[:3]).strip(\" \")\n",
    "        else:\n",
    "            ip3_start = \" \"\n",
    "            ip3_end = \" \"\n",
    "        ip3_start_col.append(ip3_start)\n",
    "        ip3_end_col.append(ip3_end)\n",
    "    df2['ip3_start'] = ip3_start_col\n",
    "    df2['ip3_end'] = ip3_end_col\n",
    "    df2 = df2[df2['ip3_start'] != \" \"]\n",
    "    # df2['ip3_start']=df2['ip3_start'].astype(float)\n",
    "    # df2['ip3_end']=df2['ip3_end'].astype(float)\n",
    "    df2.to_excel(os.path.join(path, 'filename.xlsx'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ca0d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all ip3 in a range\n",
    "def ip_to_int(ip):\n",
    "    val = 0\n",
    "    for i, s in enumerate(ip.split('.')):\n",
    "        val += int(s) * 256 ** (2 - i)\n",
    "    return val\n",
    "\n",
    "def int_to_ip(val):\n",
    "    octets = []\n",
    "    for i in range(3):\n",
    "        octets.append(str(val % 256))\n",
    "        val = val >> 8\n",
    "    return '.'.join(reversed(octets))\n",
    "\n",
    "def findIPs(start, end):\n",
    "    for i in range(ip_to_int(start), ip_to_int(end) + 1):\n",
    "        yield int_to_ip(i)\n",
    "\n",
    "if __name__== '__main__':\n",
    "    path = # add path here\n",
    "    df = pd.read_excel(os.path.join(path, 'filename.xlsx'))\n",
    "\n",
    "    ip3_list = []\n",
    "    for index, row in df.iterrows():\n",
    "        print(row[\"ip3_start\"], row[\"ip3_end\"])\n",
    "        try:\n",
    "            ip3 = list(findIPs(row[\"ip3_start\"], row[\"ip3_end\"]))\n",
    "        except:\n",
    "            ip4 = \" \"\n",
    "        print(ip3)\n",
    "        ip3_list.append(ip3)\n",
    "    df['ip4'] = ip3_list\n",
    "\n",
    "    df2=df[df['ip4']!=\" \"]\n",
    "    df2.to_excel(os.path.join(path, 'filename.xlsx'), index=False)\n",
    "\n",
    "#explode the ip3 list\n",
    "if __name__== '__main__':\n",
    "    path = # add path here\n",
    "    df = pd.read_excel(os.path.join(path, 'filename.xlsx'))\n",
    "    df2 = df.assign(ip4=df.ip4.str.split(\",\")).explode('ip4')\n",
    "    df2['ip4']=df2['ip4'].apply(lambda x: x.replace(\"[\",\"\").replace(\"]\",\"\").replace(\"'\",\"\"))\n",
    "    df2.to_excel(os.path.join(path, 'filename.xlsx'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96350e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#get EDGAR Log records associated with the truncated IP4s from 2 Terabytes data from EDGAR Log.\n",
    "def get_ip3_unique_list(df):\n",
    "    ip3_list = df['ip3_unpacked'].to_list()\n",
    "    # print(len(ip3_list)) #203485 ip3\n",
    "    ip3_set = set(ip3_list)\n",
    "    ip3_list_unique = list(ip3_set)\n",
    "    return ip3_list_unique\n",
    "\n",
    "def get_ip3_truncated(df):\n",
    "    # get ip3 truncated\n",
    "    ip_truncated = []\n",
    "    for row in df['ip']:\n",
    "        size = len(row)\n",
    "        mod_ip = row[:size - 4]\n",
    "        # print(mod_ip)\n",
    "        ip_truncated.append(mod_ip)\n",
    "        # look for ip3 of search parquet file\n",
    "    df['ip'] = ip_truncated\n",
    "    return df\n",
    "\n",
    "if __name__== '__main__':\n",
    "    path = # add path here\n",
    "    df = pd.read_excel(os.path.join(path, 'filename.xlsx'))\n",
    "    ip3_l = get_ip3_unique_list(df)\n",
    "\n",
    "if __name__== '__main__':\n",
    "    download_dir = \"./add path/\"\n",
    "    if not os.path.exists(download_dir):\n",
    "        os.makedirs(download_dir)\n",
    "\n",
    "# I have extracted about 2 Terabytes data from EDGAR Log. \n",
    "if __name__== '__main__':\n",
    "    path1 = # add path here\n",
    "    all_files = glob.glob(os.path.join(path1, \"*.parq\"))\n",
    "    for file in all_files:\n",
    "        print(file)\n",
    "        basename = os.path.basename(file)\n",
    "        filename = basename.replace(\".parq\", \"\").strip()\n",
    "        try:\n",
    "            print(\"Extracted_parquet:\", filename)\n",
    "            df = pd.read_parquet(file)\n",
    "            df2 = get_ip3_truncated(df)\n",
    "            df_filtered = df2[df2['ip'].isin(ip3_l)]\n",
    "            df_filtered.to_csv(download_dir + filename + \".csv\", index=False)\n",
    "\n",
    "        except:\n",
    "            log = os.path.join(download_dir, \"filename.txt\")\n",
    "            f_log = open(log, \"a\")\n",
    "            f_log.write('download_error:' + filename + \"\\n\")\n",
    "            f_log.close()\n",
    "            print(\"Error:\", filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
